# -*- coding: utf-8 -*-
"""FoodVisionUsingTransferLearning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wg5rAtccOxCRpOW2o-7aIc2KyaaFA8fK

# Food Vision Using EfficientNet CNN Model
"""

!nvidia-smi -L

"""### Importing some helper functions"""

!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/refs/heads/main/extras/helper_functions.py

!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/refs/heads/main/extras/helper_functions.py

"""### Downloading dataset from TensorFlow Datasets"""

import tensorflow_datasets as tfds
(train_data, test_data), ds_info = tfds.load(name="food101",
                                             split=["train", "validation"],
                                             shuffle_files=True,
                                             as_supervised=True, # data gets returned as (data, label)
                                             with_info=True)

"""### Features of Food101 dataset"""

# Features
ds_info.features

# Classes
ds_info.features["label"].names[:10]

train_data.take(1)

for image, label in train_data.take(1):
  print(image.shape)
  print(image.dtype)
  print(label)
  print(ds_info.features["label"].int2str(label))

image

import tensorflow as tf
tf.reduce_min(image), tf.reduce_max(image)

import matplotlib.pyplot as plt
plt.imshow(image)
plt.title(ds_info.features["label"].int2str(label));

"""### Preprocessing functions for our data

* Scaling the data
* Converting the data in float32 datatype to utilise mixed precision training
* Batching the tensors


"""

def preprocess_img(image, label, img_shape=224):
    '''
    Converts image datatype from uint8 -> float32 and reshapes
    image to [image_shape, image_shape, color_channels]
    '''

    image = tf.image.resize(image, [img_shape, img_shape])
    return tf.cast(image, tf.float32), label

# Testing the preprocessing function
preprocessed_image=preprocess_img(image, label)[0]
print(f"Image before preprocessing:\n {image[:2]}, {image.shape}, {image.dtype}\n")
print(f"Image after preprocessing:\n {preprocessed_image[:2]}, {preprocessed_image.shape}, {preprocessed_image.dtype}")

"""### Batching and preparing the datasets"""

train_data = train_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)
train_data = train_data.shuffle(buffer_size=1000).batch(batch_size=16).prefetch(buffer_size=tf.data.AUTOTUNE)

test_data = test_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size=16).prefetch(buffer_size=tf.data.AUTOTUNE)

train_data, test_data

"""### Creating Modelling Callbacks"""

# Tensorboard callback
from helper_functions import create_tensorboard_callback

# Model Checkpoint callback
checkpoint_path = "model_checkpoints/cp.weights.h5"
model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                         save_weights_only=True,
                                                         save_best_only=True,
                                                         verbose=0)

"""### Setting up mixed precision training"""

import tensorflow
from tensorflow.keras import mixed_precision
mixed_precision.set_global_policy("mixed_float16")

"""### Build Feature Extraction Model"""

from tensorflow.keras import layers
from tensorflow.keras import preprocessing

#Base Model
input_shape = (224, 224, 3)
base_model = tf.keras.applications.EfficientNetB0(include_top=False)
base_model.trainable = False

#Functional Model
inputs = layers.Input(shape=input_shape, name="input_layer")
x = base_model(inputs, training=False)
x  = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(len(ds_info.features["label"].names))(x)
outputs = layers.Activation("softmax", dtype=tf.float32, name="softmax_float32")(x)
model = tf.keras.Model(inputs, outputs)

#Compilation of the Model
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(),
              metrics=["accuracy"])

model.summary()

#Checking layer dtype policies
for layer in model.layers:
  print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)

for layer in model.layers:
  if(layer.name=="efficientnetb0"):
    for layer in layer.layers:
      print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)

"""### Fit the feature extraction model"""

history_101_food_classes_fe = model.fit(train_data,
                                       epochs=3,
                                       steps_per_epoch=len(train_data),
                                       validation_data=test_data,
                                       validation_steps=int(0.15 * len(test_data)),
                                       callbacks=[create_tensorboard_callback(dir_name="training_logs",
                                                                             experiment_name= "efficientnetb0_feature_extraction"),
                                                  model_checkpoint])

# Testing the feature extracted model
loss = 0.0
accuracy = 0.0
num_samples = ds_info.splits["validation"].num_examples

for batch_data, batch_label in test_data:
    batch_loss, batch_accuracy = model.test_on_batch(batch_data, batch_label)
    batch_size_actual = tf.shape(batch_data)[0]
    loss += batch_loss * tf.cast(batch_size_actual, tf.float32)
    accuracy += batch_accuracy * tf.cast(batch_size_actual, tf.float32)

loss /= num_samples
accuracy /= num_samples

print(f"Loss: {loss.numpy():.4f}")
print(f"Accuracy: {accuracy.numpy():.4f}")

"""### Finetuning the base model"""

# Unfreezing the bottom 30 layers
base_model.trainable = True
for layer in base_model.layers[:-20]:
    layer.trainable = False

# Recompile the entire model with a lower learning rate
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

# Fit the finetuned model
history_101_food_classes_ft = model.fit(train_data,
                                       epochs=5,
                                       steps_per_epoch=len(train_data),
                                       validation_data=test_data,
                                       validation_steps=int(0.15 * len(test_data)),
                                       callbacks=[create_tensorboard_callback(dir_name="training_logs",
                                                                             experiment_name= "efficientnetb0_fine_tuning"),
                                                  model_checkpoint])

# Testing the fine tuned model
import numpy as np

y_true = []
y_pred = []

loss = 0.0
accuracy = 0.0
num_samples = ds_info.splits["validation"].num_examples

for batch_data, batch_label in test_data:
    # 1. Evaluate batch
    batch_loss, batch_accuracy = model.test_on_batch(batch_data, batch_label)
    batch_size_actual = tf.shape(batch_data)[0]
    loss += batch_loss * tf.cast(batch_size_actual, tf.float32)
    accuracy += batch_accuracy * tf.cast(batch_size_actual, tf.float32)

    # 2. Collect predictions for visualization
    preds = model.predict(batch_data, verbose=0)
    pred_labels = np.argmax(preds, axis=1)

    y_true.extend(batch_label.numpy())
    y_pred.extend(pred_labels)

# 3. Normalize final scores
loss /= num_samples
accuracy /= num_samples

print(f"Loss: {loss.numpy():.4f}")
print(f"Accuracy: {accuracy.numpy():.4f}")

# Plotting the results
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score

# Convert to NumPy arrays
y_true_np = np.array(y_true)
y_pred_np = np.array(y_pred)

# Get class names
class_names = ds_info.features["label"].names
num_classes = len(class_names)

# Compute per-class accuracy
per_class_accuracy = []
for i in range(num_classes):
    mask = y_true_np == i
    if np.sum(mask) == 0:
        acc = 0  # Avoid division by zero
    else:
        acc = accuracy_score(y_true_np[mask], y_pred_np[mask])
    per_class_accuracy.append(acc)

# Create DataFrame for plotting
import pandas as pd
df = pd.DataFrame({
    "Class": class_names,
    "Accuracy": per_class_accuracy
})

# Sort by accuracy
df_sorted = df.sort_values("Accuracy", ascending=False)

# Plot
plt.figure(figsize=(12, 6))
sns.barplot(data=df_sorted, x="Class", y="Accuracy", palette="viridis")
plt.xticks(rotation=90)
plt.title("Per-Class Accuracy")
plt.ylabel("Accuracy")
plt.xlabel("Class")
plt.tight_layout()
plt.show()

# Trying to predict an image of internet
def predict_image(model, img_path, class_names, img_shape=224):
    # Load and decode the image
    img_raw = tf.io.read_file(img_path)
    img_tensor = tf.image.decode_jpeg(img_raw, channels=3)

    # Preprocess using your function (just pass a dummy label, e.g., 0)
    img_processed, _ = preprocess_img(img_tensor, label=0, img_shape=img_shape)

    # Add batch dimension
    img_processed = tf.expand_dims(img_processed, axis=0)

    # Make prediction
    preds = model.predict(img_processed)
    pred_class = tf.argmax(preds, axis=1).numpy()[0]
    pred_label = class_names[pred_class]

    return pred_label